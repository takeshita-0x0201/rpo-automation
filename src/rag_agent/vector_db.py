"""Vector Database for Recruitment RAG System"""

import os
from typing import List, Dict, Optional, Tuple
import chromadb
from chromadb.config import Settings
from langchain.vectorstores import Chroma
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
import json
from datetime import datetime


class RecruitmentVectorDB:
    """éå»ã®æ¡ç”¨è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ç®¡ç†ã™ã‚‹ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹"""
    
    def __init__(self, persist_directory: str = "./recruitment_vectors"):
        """
        Args:
            persist_directory: ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä¿å­˜å…ˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
        """
        # Embeddingãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
        self.embeddings = OpenAIEmbeddings(
            model="text-embedding-3-large",  # é«˜ç²¾åº¦ãª3072æ¬¡å…ƒ
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # ChromaDBã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ï¼ˆãƒ†ãƒ¼ãƒ–ãƒ«ï¼‰ã®ä½œæˆ
        try:
            self.collection = self.client.get_collection(
                name="recruitment_evaluations"
            )
        except:
            self.collection = self.client.create_collection(
                name="recruitment_evaluations",
                metadata={"hnsw:space": "cosine"}  # ã‚³ã‚µã‚¤ãƒ³é¡ä¼¼åº¦ã‚’ä½¿ç”¨
            )
        
        # LangChainç”¨ã®ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢
        self.vectorstore = Chroma(
            client=self.client,
            collection_name="recruitment_evaluations",
            embedding_function=self.embeddings
        )
    
    def add_evaluation(self, evaluation_data: Dict) -> str:
        """è©•ä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ """
        # ãƒ†ã‚­ã‚¹ãƒˆã®ä½œæˆ
        text = self._create_searchable_text(
            evaluation_data["job_requirement"],
            evaluation_data["candidate_resume"]
        )
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®æº–å‚™
        metadata = {
            "id": evaluation_data["id"],
            "final_grade": evaluation_data["human_review"]["final_grade"],
            "final_score": evaluation_data["human_review"]["final_score"],
            "positive_summary": self._summarize_reasons(
                evaluation_data["ai_evaluation"]["positive_reasons"]
            ),
            "concern_summary": self._summarize_reasons(
                evaluation_data["ai_evaluation"]["concerns"]
            ),
            "decision": evaluation_data["human_review"]["decision"],
            "job_id": evaluation_data["job_requirement"].get("id"),
            "candidate_id": evaluation_data["candidate_resume"].get("id"),
            "reviewer": evaluation_data["human_review"]["reviewer"],
            "timestamp": evaluation_data["timestamp"],
            "job_title": evaluation_data["job_requirement"]["title"],
            "company": evaluation_data["job_requirement"]["company"]
        }
        
        # ãƒ™ã‚¯ãƒˆãƒ«DBã«è¿½åŠ 
        self.vectorstore.add_texts(
            texts=[text],
            metadatas=[metadata],
            ids=[evaluation_data["id"]]
        )
        
        return evaluation_data["id"]
    
    def search_similar_cases(
        self, 
        query: str, 
        k: int = 3,
        score_threshold: float = 0.7
    ) -> List[Tuple[Document, float]]:
        """é¡ä¼¼ã™ã‚‹éå»ã®è©•ä¾¡äº‹ä¾‹ã‚’æ¤œç´¢"""
        results = self.vectorstore.similarity_search_with_score(
            query=query,
            k=k
        )
        
        # ã‚¹ã‚³ã‚¢ã—ãã„å€¤ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_results = [
            (doc, score) for doc, score in results 
            if (1 - score) >= score_threshold
        ]
        
        return filtered_results
    
    def _create_searchable_text(self, job_req: Dict, resume: Dict) -> str:
        """æ¤œç´¢ç”¨ã®ãƒ†ã‚­ã‚¹ãƒˆã‚’ç”Ÿæˆ"""
        return f"""
ã€æ¡ç”¨è¦ä»¶ã€‘
è·ç¨®: {job_req.get('title', '')}
ä¼šç¤¾: {job_req.get('company', '')}
å¿…é ˆã‚¹ã‚­ãƒ«: {', '.join(job_req.get('required_skills', []))}
æ­“è¿ã‚¹ã‚­ãƒ«: {', '.join(job_req.get('preferred_skills', []))}
çµŒé¨“å¹´æ•°: {job_req.get('experience_years', '')}
ãƒãƒ¼ãƒ è¦æ¨¡: {job_req.get('team_size', '')}
å¹´åãƒ¬ãƒ³ã‚¸: {job_req.get('salary_range', '')}
æ¥­å‹™å†…å®¹: {job_req.get('description', '')[:500]}

ã€å€™è£œè€…æƒ…å ±ã€‘
åå‰: {resume.get('name', '')}
çµŒé¨“å¹´æ•°: {resume.get('experience', '')}
ç¾åœ¨ã®å½¹è·: {resume.get('current_position', '')}
ã‚¹ã‚­ãƒ«: {', '.join(resume.get('skills', []))}
è·æ­´: {resume.get('work_history', '')[:500]}
å®Ÿç¸¾: {resume.get('achievements', '')[:500]}
        """
    
    def _summarize_reasons(self, reasons: List[str]) -> str:
        """ç†ç”±ã®ãƒªã‚¹ãƒˆã‚’è¦ç´„"""
        if not reasons:
            return ""
        return " / ".join(reasons[:3])  # æœ€åˆã®3ã¤ã¾ã§
    
    def get_statistics(self) -> Dict:
        """ãƒ™ã‚¯ãƒˆãƒ«DBã®çµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        total_count = self.collection.count()
        
        # ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ¥ã®é›†è¨ˆï¼ˆç°¡æ˜“ç‰ˆï¼‰
        all_data = self.collection.get()
        grade_counts = {"A": 0, "B": 0, "C": 0, "D": 0}
        
        if all_data and "metadatas" in all_data:
            for metadata in all_data["metadatas"]:
                grade = metadata.get("final_grade")
                if grade in grade_counts:
                    grade_counts[grade] += 1
        
        return {
            "total_evaluations": total_count,
            "grade_distribution": grade_counts,
            "last_updated": datetime.now().isoformat()
        }
    
    def reset_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆï¼ˆé–‹ç™ºç”¨ï¼‰"""
        self.client.delete_collection("recruitment_evaluations")
        self.collection = self.client.create_collection(
            name="recruitment_evaluations",
            metadata={"hnsw:space": "cosine"}
        )
        print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã—ãŸ")


# åˆæœŸåŒ–ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
if __name__ == "__main__":
    # ç’°å¢ƒå¤‰æ•°ã®ç¢ºèª
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ OPENAI_API_KEYãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        exit(1)
    
    # ãƒ™ã‚¯ãƒˆãƒ«DBã®åˆæœŸåŒ–
    db = RecruitmentVectorDB()
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã®è¿½åŠ ï¼ˆãƒ†ã‚¹ãƒˆç”¨ï¼‰
    sample_evaluation = {
        "id": "eval_sample_001",
        "timestamp": datetime.now().isoformat(),
        "job_requirement": {
            "id": "job_001",
            "title": "ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
            "company": "æ ªå¼ä¼šç¤¾ã‚µãƒ³ãƒ—ãƒ«",
            "required_skills": ["Python", "FastAPI", "AWS"],
            "preferred_skills": ["Docker", "Kubernetes"],
            "experience_years": "3å¹´ä»¥ä¸Š",
            "team_size": "5å",
            "salary_range": "600-800ä¸‡å††",
            "description": "ECã‚µã‚¤ãƒˆã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™º"
        },
        "candidate_resume": {
            "id": "cand_001",
            "name": "ãƒ†ã‚¹ãƒˆå€™è£œè€…",
            "experience": "5å¹´",
            "current_position": "ã‚·ãƒ‹ã‚¢ã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢",
            "skills": ["Python", "Django", "AWS", "Docker"],
            "work_history": "Webç³»ä¼æ¥­ã§ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰é–‹ç™ºã«å¾“äº‹",
            "achievements": "ãƒã‚¤ã‚¯ãƒ­ã‚µãƒ¼ãƒ“ã‚¹åŒ–ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚’ãƒªãƒ¼ãƒ‰"
        },
        "ai_evaluation": {
            "score": 85,
            "grade": "A",
            "positive_reasons": [
                "å¿…é ˆã‚¹ã‚­ãƒ«ã‚’å…¨ã¦æº€ãŸã—ã¦ã„ã‚‹",
                "çµŒé¨“å¹´æ•°ãŒè¦ä»¶ã‚’ä¸Šå›ã‚‹"
            ],
            "concerns": ["FastAPIçµŒé¨“ãŒæ˜è¨˜ã•ã‚Œã¦ã„ãªã„"]
        },
        "human_review": {
            "final_score": 88,
            "final_grade": "A",
            "reviewer": "æ¡ç”¨ãƒãƒãƒ¼ã‚¸ãƒ£ãƒ¼",
            "comments": "æŠ€è¡“åŠ›ã¯ç”³ã—åˆ†ãªã—",
            "decision": "æ¬¡é¸è€ƒã¸é€²ã‚€"
        }
    }
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ 
    doc_id = db.add_evaluation(sample_evaluation)
    print(f"âœ… ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ã—ã¾ã—ãŸ: {doc_id}")
    
    # çµ±è¨ˆæƒ…å ±ã®è¡¨ç¤º
    stats = db.get_statistics()
    print(f"\nğŸ“Š ãƒ™ã‚¯ãƒˆãƒ«DBçµ±è¨ˆæƒ…å ±:")
    print(f"  - ç·è©•ä¾¡æ•°: {stats['total_evaluations']}")
    print(f"  - ã‚°ãƒ¬ãƒ¼ãƒ‰åˆ†å¸ƒ: {stats['grade_distribution']}")