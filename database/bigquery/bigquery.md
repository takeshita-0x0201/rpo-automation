# BigQueryã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‚¬ã‚¤ãƒ‰

ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€RPOè‡ªå‹•åŒ–ã‚·ã‚¹ãƒ†ãƒ ã§ä½¿ç”¨ã™ã‚‹BigQueryãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®è©³ç´°ãªã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ‰‹é †ã‚’èª¬æ˜ã—ã¾ã™ã€‚

## ç›®æ¬¡

- [BigQuery APIã®æœ‰åŠ¹åŒ–](#bigquery-apiã®æœ‰åŠ¹åŒ–)
- [ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ](#ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ)
- [ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ](#ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ)
- [ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®è¨­å®š](#ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®è¨­å®š)
- [æ¥ç¶šãƒ†ã‚¹ãƒˆ](#æ¥ç¶šãƒ†ã‚¹ãƒˆ)
- [ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°](#ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°)
- [ã‚¹ãƒˆã‚¢ãƒ‰ãƒ—ãƒ­ã‚·ãƒ¼ã‚¸ãƒ£ã®ä½œæˆ](#ã‚¹ãƒˆã‚¢ãƒ‰ãƒ—ãƒ­ã‚·ãƒ¼ã‚¸ãƒ£ã®ä½œæˆ)
- [ã‚³ã‚¹ãƒˆç®¡ç†](#ã‚³ã‚¹ãƒˆç®¡ç†)
- [ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ](#ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ)

---

## BigQuery APIã®æœ‰åŠ¹åŒ–

### 1. Google Cloud Consoleã§ã®è¨­å®š

```bash
# gcloud CLIã§ä¸€æ‹¬æœ‰åŠ¹åŒ–
gcloud services enable \
    bigquery.googleapis.com \
    bigquerydatatransfer.googleapis.com \
    cloudresourcemanager.googleapis.com \
    logging.googleapis.com \
    monitoring.googleapis.com
```

### 2. æœ‰åŠ¹åŒ–ã®ç¢ºèª

```bash
# æœ‰åŠ¹åŒ–ã•ã‚ŒãŸAPIã®ç¢ºèª
gcloud services list --enabled --filter="name:bigquery"
```

---

## ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ä½œæˆ

### 1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹æˆã®è¨­è¨ˆ

æœ¬ã‚·ã‚¹ãƒ†ãƒ ã§ã¯ä»¥ä¸‹ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæ§‹æˆã‚’ä½¿ç”¨ã—ã¾ã™ï¼š

```
your-project/
â”œâ”€â”€ rpo_raw_data/          # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ç”Ÿãƒ‡ãƒ¼ã‚¿
â”œâ”€â”€ rpo_structured_data/   # AIæ§‹é€ åŒ–æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿  
â”œâ”€â”€ rpo_matching_results/  # AIåˆ¤å®šçµæœ
â””â”€â”€ system_logs/           # ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ãƒ»ã‚¨ãƒ©ãƒ¼æƒ…å ±
```

### 2. gcloudã‚³ãƒãƒ³ãƒ‰ã§ã®ä½œæˆ

```bash
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’è¨­å®š
export PROJECT_ID="your-project-id"
export LOCATION="asia-northeast1"  # æ±äº¬ãƒªãƒ¼ã‚¸ãƒ§ãƒ³

# å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆ
bq mk --location=$LOCATION --dataset $PROJECT_ID:rpo_raw_data
bq mk --location=$LOCATION --dataset $PROJECT_ID:rpo_structured_data  
bq mk --location=$LOCATION --dataset $PROJECT_ID:rpo_matching_results
bq mk --location=$LOCATION --dataset $PROJECT_ID:system_logs

# ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã®ç¢ºèª
bq ls --max_results=10
```

### 3. Google Cloud Consoleã§ã®ä½œæˆ

1. [BigQuery Console](https://console.cloud.google.com/bigquery)ã‚’é–‹ã
2. ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆåã®å³å´ã®ã€Œï¸™ã€â†’ã€Œãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã€
3. å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä»¥ä¸‹ã®è¨­å®šã§ä½œæˆï¼š

```
ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆID: rpo_raw_data
ãƒ‡ãƒ¼ã‚¿ã®ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³: asia-northeast1 (Tokyo)
ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ†ãƒ¼ãƒ–ãƒ«æœ‰åŠ¹æœŸé™: ãªã—
æš—å·åŒ–: Googleç®¡ç†ã®æš—å·åŒ–ã‚­ãƒ¼
```

---

## ãƒ†ãƒ¼ãƒ–ãƒ«ã®ä½œæˆ

### 1. raw_data ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ãƒ¼ãƒ–ãƒ«

#### raw_candidatesï¼ˆå€™è£œè€…ç”Ÿãƒ‡ãƒ¼ã‚¿ï¼‰

```sql
CREATE TABLE `{project_id}.rpo_raw_data.raw_candidates` (
    scraped_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    session_id STRING NOT NULL,
    client_id STRING NOT NULL,
    client_name STRING,
    candidate_url STRING NOT NULL,
    raw_html STRING,
    raw_data JSON,
    scraped_by_user_id STRING,
    ingestion_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP()
)
PARTITION BY DATE(scraped_at)
CLUSTER BY client_id, session_id
OPTIONS (
    description = "Bizreachã‹ã‚‰ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã—ãŸå€™è£œè€…ã®ç”Ÿãƒ‡ãƒ¼ã‚¿",
    partition_expiration_days = 90
);
```

#### raw_scraping_sessionsï¼ˆã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼‰

```sql
CREATE TABLE `{project_id}.rpo_raw_data.raw_scraping_sessions` (
    session_id STRING NOT NULL,
    user_id STRING NOT NULL,
    client_id STRING NOT NULL,
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    total_candidates_found INTEGER DEFAULT 0,
    status STRING DEFAULT 'running',
    error_message STRING,
    session_metadata JSON
)
PARTITION BY DATE(started_at)
CLUSTER BY user_id, client_id;
```

### 2. structured_data ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ãƒ¼ãƒ–ãƒ«

#### structured_candidatesï¼ˆæ§‹é€ åŒ–æ¸ˆã¿å€™è£œè€…ãƒ‡ãƒ¼ã‚¿ï¼‰

```sql
CREATE TABLE `{project_id}.rpo_structured_data.structured_candidates` (
    candidate_id STRING NOT NULL,
    original_url STRING NOT NULL,
    client_id STRING NOT NULL,
    name STRING,
    email STRING,
    phone STRING,
    current_company STRING,
    current_position STRING,
    skills ARRAY<STRING>,
    experience_years INTEGER,
    education STRING,
    location STRING,
    salary_expectation STRUCT<
        min INTEGER,
        max INTEGER,
        currency STRING
    >,
    structured_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    structured_data JSON,
    structuring_model STRING DEFAULT 'gemini-pro',
    confidence_score FLOAT64
)
PARTITION BY DATE(structured_at)
CLUSTER BY client_id, current_company
OPTIONS (
    description = "AIã«ã‚ˆã£ã¦æ§‹é€ åŒ–ã•ã‚ŒãŸå€™è£œè€…ãƒ‡ãƒ¼ã‚¿"
);
```

#### job_requirementsï¼ˆæ¡ç”¨è¦ä»¶ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆï¼‰

```sql
CREATE TABLE `{project_id}.rpo_structured_data.job_requirements` (
    id STRING NOT NULL,
    client_id STRING NOT NULL,
    title STRING NOT NULL,
    position STRING,
    description STRING,
    required_skills ARRAY<STRING>,
    preferred_skills ARRAY<STRING>,
    experience_years INTEGER,
    education_level STRING,
    salary_range STRUCT<
        min INTEGER,
        max INTEGER,
        currency STRING
    >,
    work_location STRING,
    employment_type STRING,
    original_text STRING,
    structured_data JSON,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    created_by STRING
)
PARTITION BY DATE(created_at)
CLUSTER BY client_id;
```

### 3. matching_results ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ãƒ¼ãƒ–ãƒ«

#### matching_resultsï¼ˆãƒãƒƒãƒãƒ³ã‚°çµæœï¼‰

```sql
CREATE TABLE `{project_id}.rpo_matching_results.matching_results` (
    result_id STRING NOT NULL,
    job_id STRING NOT NULL,
    candidate_id STRING NOT NULL,
    client_id STRING NOT NULL,
    requirement_id STRING NOT NULL,
    match_score FLOAT64,
    match_reasons ARRAY<STRING>,
    concerns ARRAY<STRING>,
    ai_evaluation JSON,
    evaluated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    evaluation_model STRING DEFAULT 'gpt-4',
    model_version STRING,
    processing_time_seconds FLOAT64
)
PARTITION BY DATE(evaluated_at)
CLUSTER BY client_id, job_id, match_score DESC
OPTIONS (
    description = "AIã«ã‚ˆã‚‹å€™è£œè€…ãƒãƒƒãƒãƒ³ã‚°çµæœ"
);
```

#### evaluation_summariesï¼ˆè©•ä¾¡ã‚µãƒãƒªãƒ¼ï¼‰

```sql
CREATE TABLE `{project_id}.rpo_matching_results.evaluation_summaries` (
    job_id STRING NOT NULL,
    client_id STRING NOT NULL,
    requirement_id STRING NOT NULL,
    total_candidates INTEGER,
    high_match_count INTEGER,    -- ã‚¹ã‚³ã‚¢80ä»¥ä¸Š
    medium_match_count INTEGER,  -- ã‚¹ã‚³ã‚¢60-79
    low_match_count INTEGER,     -- ã‚¹ã‚³ã‚¢60æœªæº€
    avg_match_score FLOAT64,
    evaluation_completed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    summary_data JSON
)
PARTITION BY DATE(evaluation_completed_at)
CLUSTER BY client_id;
```

### 4. system_logs ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ãƒ¼ãƒ–ãƒ«

#### application_logsï¼ˆã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ­ã‚°ï¼‰

```sql
CREATE TABLE `{project_id}.system_logs.application_logs` (
    log_id STRING NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    level STRING,  -- INFO, WARNING, ERROR, CRITICAL
    source STRING, -- web_app, chrome_extension, cloud_function
    message STRING,
    user_id STRING,
    session_id STRING,
    error_details JSON,
    stack_trace STRING
)
PARTITION BY DATE(timestamp)
CLUSTER BY level, source
OPTIONS (
    partition_expiration_days = 30
);
```

#### api_access_logsï¼ˆAPI ã‚¢ã‚¯ã‚»ã‚¹ãƒ­ã‚°ï¼‰

```sql
CREATE TABLE `{project_id}.system_logs.api_access_logs` (
    request_id STRING NOT NULL,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP(),
    method STRING,
    endpoint STRING,
    user_id STRING,
    ip_address STRING,
    user_agent STRING,
    response_status INTEGER,
    response_time_ms INTEGER,
    request_size_bytes INTEGER,
    response_size_bytes INTEGER
)
PARTITION BY DATE(timestamp)
CLUSTER BY endpoint, response_status;
```

### 5. ä¸€æ‹¬ãƒ†ãƒ¼ãƒ–ãƒ«ä½œæˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

```bash
#!/bin/bash
# create_bigquery_tables.sh

PROJECT_ID="your-project-id"

# SQLãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª
SQL_DIR="./sql/bigquery"

# å„SQLãƒ•ã‚¡ã‚¤ãƒ«ã‚’å®Ÿè¡Œ
for sql_file in $SQL_DIR/*.sql; do
    echo "Executing $sql_file..."
    
    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆIDã‚’ç½®æ›ã—ã¦ã‚¯ã‚¨ãƒªå®Ÿè¡Œ
    sed "s/{project_id}/$PROJECT_ID/g" "$sql_file" | bq query --use_legacy_sql=false
    
    if [ $? -eq 0 ]; then
        echo "âœ… Successfully executed $sql_file"
    else
        echo "âŒ Failed to execute $sql_file"
        exit 1
    fi
done

echo "ğŸ‰ All tables created successfully!"
```

---

## ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®è¨­å®š

### 1. ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã®ä½œæˆ

```bash
# BigQueryå°‚ç”¨ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚’ä½œæˆ
gcloud iam service-accounts create rpo-automation-bigquery \
    --display-name="RPO Automation BigQuery Service Account" \
    --description="Service account for RPO automation system BigQuery access"
```

### 2. æ¨©é™ã®ä»˜ä¸

```bash
PROJECT_ID="your-project-id"
SERVICE_ACCOUNT="rpo-automation-bigquery@${PROJECT_ID}.iam.gserviceaccount.com"

# BigQuery Data Editorï¼ˆãƒ‡ãƒ¼ã‚¿ã®èª­ã¿æ›¸ãï¼‰
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member="serviceAccount:$SERVICE_ACCOUNT" \
    --role="roles/bigquery.dataEditor"

# BigQuery Job Userï¼ˆã‚¯ã‚¨ãƒªå®Ÿè¡Œï¼‰
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member="serviceAccount:$SERVICE_ACCOUNT" \
    --role="roles/bigquery.jobUser"

# Logging Writerï¼ˆãƒ­ã‚°å‡ºåŠ›ï¼‰
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member="serviceAccount:$SERVICE_ACCOUNT" \
    --role="roles/logging.logWriter"
```

### 3. ã‚­ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ

```bash
# èªè¨¼ã‚­ãƒ¼ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
gcloud iam service-accounts keys create ./credentials/bigquery-service-account.json \
    --iam-account=$SERVICE_ACCOUNT

# æ¨©é™ã®ç¢ºèª
chmod 600 ./credentials/bigquery-service-account.json
```

### 4. æœ€å°æ¨©é™ã®åŸå‰‡ã«ã‚ˆã‚‹ã‚«ã‚¹ã‚¿ãƒ ãƒ­ãƒ¼ãƒ«

```bash
# ã‚«ã‚¹ã‚¿ãƒ ãƒ­ãƒ¼ãƒ«ã‚’ä½œæˆï¼ˆæ¨å¥¨ï¼‰
gcloud iam roles create rpo.bigquery.limited \
    --project=$PROJECT_ID \
    --title="RPO BigQuery Limited Access" \
    --description="Limited BigQuery access for RPO automation" \
    --permissions=bigquery.datasets.get,bigquery.tables.get,bigquery.tables.list,bigquery.tables.create,bigquery.tables.update,bigquery.tables.getData,bigquery.tables.updateData,bigquery.jobs.create

# ã‚«ã‚¹ã‚¿ãƒ ãƒ­ãƒ¼ãƒ«ã‚’é©ç”¨
gcloud projects add-iam-policy-binding $PROJECT_ID \
    --member="serviceAccount:$SERVICE_ACCOUNT" \
    --role="projects/$PROJECT_ID/roles/rpo.bigquery.limited"
```

---

## æ¥ç¶šãƒ†ã‚¹ãƒˆ

### 1. Python ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã§ã®æ¥ç¶šãƒ†ã‚¹ãƒˆ

```python
# test_bigquery_connection.py
import os
from google.cloud import bigquery
from google.oauth2 import service_account
from dotenv import load_dotenv

def test_bigquery_connection():
    load_dotenv()
    
    # èªè¨¼æƒ…å ±ã®è¨­å®š
    credentials_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    project_id = os.getenv("BIGQUERY_PROJECT_ID")
    
    print(f"Project ID: {project_id}")
    print(f"Credentials: {credentials_path}")
    
    try:
        # ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        credentials = service_account.Credentials.from_service_account_file(
            credentials_path
        )
        client = bigquery.Client(
            credentials=credentials,
            project=project_id
        )
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸€è¦§ã®å–å¾—
        datasets = list(client.list_datasets())
        print(f"âœ… Connection successful!")
        print(f"Found {len(datasets)} datasets:")
        
        for dataset in datasets:
            print(f"  - {dataset.dataset_id}")
            
            # å„ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒ†ãƒ¼ãƒ–ãƒ«ä¸€è¦§
            dataset_ref = client.dataset(dataset.dataset_id)
            tables = list(client.list_tables(dataset_ref))
            print(f"    Tables: {len(tables)}")
            
            for table in tables[:3]:  # æœ€åˆã®3ã¤ã®ãƒ†ãƒ¼ãƒ–ãƒ«ã®ã¿è¡¨ç¤º
                print(f"      - {table.table_id}")
        
        # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®å®Ÿè¡Œ
        test_query = """
        SELECT 
            table_name,
            table_type,
            creation_time
        FROM `{}.rpo_raw_data.INFORMATION_SCHEMA.TABLES`
        LIMIT 5
        """.format(project_id)
        
        query_job = client.query(test_query)
        results = query_job.result()
        
        print("\nâœ… Test query successful!")
        print("Tables in rpo_raw_data:")
        for row in results:
            print(f"  - {row.table_name} ({row.table_type})")
            
    except Exception as e:
        print(f"âŒ Connection failed: {e}")
        import traceback
        traceback.print_exc()

def test_data_insertion():
    """ãƒ‡ãƒ¼ã‚¿æŒ¿å…¥ãƒ†ã‚¹ãƒˆ"""
    load_dotenv()
    
    credentials_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
    project_id = os.getenv("BIGQUERY_PROJECT_ID")
    
    credentials = service_account.Credentials.from_service_account_file(
        credentials_path
    )
    client = bigquery.Client(credentials=credentials, project=project_id)
    
    # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æŒ¿å…¥
    table_id = f"{project_id}.system_logs.application_logs"
    
    rows_to_insert = [
        {
            "log_id": "test-001",
            "level": "INFO",
            "source": "test_script",
            "message": "BigQuery connection test successful",
            "user_id": "test-user"
        }
    ]
    
    try:
        errors = client.insert_rows_json(table_id, rows_to_insert)
        if errors:
            print(f"âŒ Insert errors: {errors}")
        else:
            print("âœ… Test data inserted successfully!")
            
    except Exception as e:
        print(f"âŒ Insert failed: {e}")

if __name__ == "__main__":
    test_bigquery_connection()
    test_data_insertion()
```

### 2. å®Ÿè¡Œæ–¹æ³•

```bash
# ç’°å¢ƒå¤‰æ•°ã‚’è¨­å®š
export GOOGLE_APPLICATION_CREDENTIALS="./credentials/bigquery-service-account.json"
export BIGQUERY_PROJECT_ID="your-project-id"

# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
python test_bigquery_connection.py
```

---

## ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã¨ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°

### 1. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°æˆ¦ç•¥

å„ãƒ†ãƒ¼ãƒ–ãƒ«ã®æ™‚ç³»åˆ—ãƒ‡ãƒ¼ã‚¿ã«å¯¾ã—ã¦ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ã‚’é©ç”¨ï¼š

```sql
-- æ—¥ä»˜ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆæ¨å¥¨ï¼‰
PARTITION BY DATE(scraped_at)

-- æœˆå˜ä½ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆå¤§é‡ãƒ‡ãƒ¼ã‚¿ã®å ´åˆï¼‰
PARTITION BY DATE_TRUNC(scraped_at, MONTH)

-- ç¯„å›²ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ‹ãƒ³ã‚°ï¼ˆæ•°å€¤ã‚«ãƒ©ãƒ ï¼‰
PARTITION BY RANGE_BUCKET(match_score, GENERATE_ARRAY(0, 100, 10))
```

### 2. ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°æˆ¦ç•¥

é »ç¹ã«ä½¿ç”¨ã•ã‚Œã‚‹æ¤œç´¢æ¡ä»¶ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒªãƒ³ã‚°ã‚’è¨­å®šï¼š

```sql
-- å€™è£œè€…ãƒ‡ãƒ¼ã‚¿: client_id â†’ session_id
CLUSTER BY client_id, session_id

-- ãƒãƒƒãƒãƒ³ã‚°çµæœ: client_id â†’ job_id â†’ match_score (é™é †)
CLUSTER BY client_id, job_id, match_score DESC

-- ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿: level â†’ source
CLUSTER BY level, source
```

### 3. ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ç®¡ç†ã®è‡ªå‹•åŒ–

```sql
-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æœ‰åŠ¹æœŸé™ã®è¨­å®š
ALTER TABLE `{project_id}.rpo_raw_data.raw_candidates` 
SET OPTIONS (partition_expiration_days = 90);

-- å¤ã„ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ã®æ‰‹å‹•å‰Šé™¤
DROP TABLE `{project_id}.rpo_raw_data.raw_candidates$20231201`;
```

---

## ã‚¹ãƒˆã‚¢ãƒ‰ãƒ—ãƒ­ã‚·ãƒ¼ã‚¸ãƒ£ã®ä½œæˆ

### 1. ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯ç”¨ãƒ—ãƒ­ã‚·ãƒ¼ã‚¸ãƒ£

```sql
-- ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
CREATE OR REPLACE PROCEDURE `{project_id}.system_logs.check_data_quality`()
BEGIN
    DECLARE candidate_duplicates INT64;
    DECLARE missing_urls INT64;
    
    -- é‡è¤‡å€™è£œè€…ã®ãƒã‚§ãƒƒã‚¯
    SELECT COUNT(*) INTO candidate_duplicates
    FROM (
        SELECT candidate_url, COUNT(*) as cnt
        FROM `{project_id}.rpo_structured_data.structured_candidates`
        GROUP BY candidate_url
        HAVING cnt > 1
    );
    
    -- å¿…é ˆãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã®æ¬ æãƒã‚§ãƒƒã‚¯
    SELECT COUNT(*) INTO missing_urls
    FROM `{project_id}.rpo_structured_data.structured_candidates`
    WHERE candidate_url IS NULL OR candidate_url = '';
    
    -- ãƒ­ã‚°ã«çµæœã‚’è¨˜éŒ²
    INSERT INTO `{project_id}.system_logs.application_logs`
    (log_id, level, source, message)
    VALUES
    (GENERATE_UUID(), 'INFO', 'data_quality_check', 
     FORMAT('Quality check completed. Duplicates: %d, Missing URLs: %d', 
            candidate_duplicates, missing_urls));
    
    -- çµæœã‚’è¿”ã™
    SELECT 
        candidate_duplicates as duplicate_candidates,
        missing_urls as missing_required_fields,
        CURRENT_TIMESTAMP() as checked_at;
END;
```

### 2. å¤ã„ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ç”¨ãƒ—ãƒ­ã‚·ãƒ¼ã‚¸ãƒ£

```sql
-- å¤ã„ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–
CREATE OR REPLACE PROCEDURE `{project_id}.system_logs.archive_old_logs`(
    retention_days INT64 DEFAULT 30
)
BEGIN
    DECLARE deleted_count INT64;
    
    -- 30æ—¥ä»¥ä¸Šå¤ã„ãƒ­ã‚°ã‚’å‰Šé™¤
    DELETE FROM `{project_id}.system_logs.application_logs`
    WHERE DATE(timestamp) < DATE_SUB(CURRENT_DATE(), INTERVAL retention_days DAY);
    
    SET deleted_count = @@row_count;
    
    -- ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å®Œäº†ãƒ­ã‚°
    INSERT INTO `{project_id}.system_logs.application_logs`
    (log_id, level, source, message)
    VALUES
    (GENERATE_UUID(), 'INFO', 'archive_procedure', 
     FORMAT('Archived %d old log entries (older than %d days)', 
            deleted_count, retention_days));
END;
```

### 3. çµ±è¨ˆæƒ…å ±æ›´æ–°ç”¨ãƒ—ãƒ­ã‚·ãƒ¼ã‚¸ãƒ£

```sql
-- ãƒãƒƒãƒãƒ³ã‚°çµ±è¨ˆã®æ›´æ–°
CREATE OR REPLACE PROCEDURE `{project_id}.rpo_matching_results.update_evaluation_summaries`(
    job_id STRING
)
BEGIN
    -- æ—¢å­˜ã‚µãƒãƒªãƒ¼ã‚’å‰Šé™¤
    DELETE FROM `{project_id}.rpo_matching_results.evaluation_summaries`
    WHERE job_id = job_id;
    
    -- æ–°ã—ã„ã‚µãƒãƒªãƒ¼ã‚’è¨ˆç®—ãƒ»æŒ¿å…¥
    INSERT INTO `{project_id}.rpo_matching_results.evaluation_summaries`
    (job_id, client_id, requirement_id, total_candidates, 
     high_match_count, medium_match_count, low_match_count, avg_match_score)
    SELECT 
        job_id,
        client_id,
        requirement_id,
        COUNT(*) as total_candidates,
        SUM(CASE WHEN match_score >= 80 THEN 1 ELSE 0 END) as high_match_count,
        SUM(CASE WHEN match_score >= 60 AND match_score < 80 THEN 1 ELSE 0 END) as medium_match_count,
        SUM(CASE WHEN match_score < 60 THEN 1 ELSE 0 END) as low_match_count,
        AVG(match_score) as avg_match_score
    FROM `{project_id}.rpo_matching_results.matching_results`
    WHERE job_id = job_id
    GROUP BY job_id, client_id, requirement_id;
END;
```

---

## ã‚³ã‚¹ãƒˆç®¡ç†

### 1. ã‚¯ã‚¨ãƒªã‚³ã‚¹ãƒˆã®ç›£è¦–

```sql
-- é«˜ã‚³ã‚¹ãƒˆã‚¯ã‚¨ãƒªã®ç¢ºèª
SELECT
    job_id,
    user_email,
    query,
    total_bytes_processed,
    total_bytes_billed,
    total_slot_ms,
    creation_time
FROM `region-asia-northeast1`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)
    AND total_bytes_billed > 1000000000  -- 1GBä»¥ä¸Š
ORDER BY total_bytes_billed DESC
LIMIT 10;
```

### 2. æœˆé–“ä½¿ç”¨é‡ã®ç¢ºèª

```sql
-- æœˆé–“ã‚¯ã‚¨ãƒªä½¿ç”¨é‡
SELECT
    DATE_TRUNC(creation_time, MONTH) as month,
    SUM(total_bytes_processed) / (1024*1024*1024) as total_gb_processed,
    SUM(total_bytes_billed) / (1024*1024*1024) as total_gb_billed,
    COUNT(*) as query_count
FROM `region-asia-northeast1`.INFORMATION_SCHEMA.JOBS_BY_PROJECT
WHERE creation_time >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 6 MONTH)
    AND job_type = 'QUERY'
GROUP BY month
ORDER BY month DESC;
```

### 3. ã‚³ã‚¹ãƒˆæœ€é©åŒ–ã®ãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹

```sql
-- âŒ æ‚ªã„ä¾‹: å…¨ã‚«ãƒ©ãƒ å–å¾—
SELECT *
FROM `{project_id}.rpo_structured_data.structured_candidates`
WHERE DATE(structured_at) = CURRENT_DATE();

-- âœ… è‰¯ã„ä¾‹: å¿…è¦ãªã‚«ãƒ©ãƒ ã®ã¿
SELECT candidate_id, name, current_company, skills
FROM `{project_id}.rpo_structured_data.structured_candidates`
WHERE DATE(structured_at) = CURRENT_DATE();

-- âœ… ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³æ´»ç”¨
SELECT candidate_id, name, current_company
FROM `{project_id}.rpo_structured_data.structured_candidates`
WHERE structured_at >= TIMESTAMP('2023-12-01')
    AND structured_at < TIMESTAMP('2023-12-02');
```

### 4. ã‚¯ã‚¨ãƒªå®Ÿè¡Œå‰ã®ã‚³ã‚¹ãƒˆè¦‹ç©ã‚‚ã‚Š

```python
def estimate_query_cost(client, query):
    """ã‚¯ã‚¨ãƒªã®å®Ÿè¡Œã‚³ã‚¹ãƒˆã‚’è¦‹ç©ã‚‚ã‚Š"""
    job_config = bigquery.QueryJobConfig(dry_run=True, use_query_cache=False)
    
    job = client.query(query, job_config=job_config)
    
    bytes_processed = job.total_bytes_processed
    gb_processed = bytes_processed / (1024**3)
    estimated_cost_usd = gb_processed * 0.005  # $5 per TB
    
    print(f"Estimated bytes processed: {bytes_processed:,}")
    print(f"Estimated GB processed: {gb_processed:.2f}")
    print(f"Estimated cost: ${estimated_cost_usd:.4f}")
    
    return gb_processed
```

---

## ç›£è¦–ã¨ã‚¢ãƒ©ãƒ¼ãƒˆ

### 1. Cloud Monitoringã§ã®ç›£è¦–è¨­å®š

```bash
# ã‚«ã‚¹ã‚¿ãƒ ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®ä½œæˆ
gcloud logging metrics create bigquery_error_rate \
    --description="BigQuery error rate" \
    --log-filter='resource.type="bigquery_resource" AND severity="ERROR"'
```

### 2. ã‚¢ãƒ©ãƒ¼ãƒˆãƒãƒªã‚·ãƒ¼ã®è¨­å®š

```yaml
# alert-policy.yaml
displayName: "BigQuery High Error Rate"
conditions:
  - displayName: "Error rate exceeds threshold"
    conditionThreshold:
      filter: 'metric.type="logging.googleapis.com/user/bigquery_error_rate"'
      comparison: COMPARISON_GREATER_THAN
      thresholdValue: 5
      duration: 300s
notificationChannels:
  - "projects/your-project/notificationChannels/your-channel-id"
```

### 3. ãƒ‡ãƒ¼ã‚¿å“è³ªç›£è¦–

```sql
-- æ¯æ—¥ã®è‡ªå‹•ãƒ‡ãƒ¼ã‚¿å“è³ªãƒã‚§ãƒƒã‚¯
CREATE OR REPLACE TABLE FUNCTION `{project_id}.system_logs.daily_quality_check`()
RETURNS TABLE<metric_name STRING, metric_value INT64, check_date DATE>
AS (
    SELECT 'total_candidates' as metric_name, 
           COUNT(*) as metric_value,
           CURRENT_DATE() as check_date
    FROM `{project_id}.rpo_structured_data.structured_candidates`
    WHERE DATE(structured_at) = CURRENT_DATE()
    
    UNION ALL
    
    SELECT 'missing_skills' as metric_name,
           COUNT(*) as metric_value, 
           CURRENT_DATE() as check_date
    FROM `{project_id}.rpo_structured_data.structured_candidates`
    WHERE DATE(structured_at) = CURRENT_DATE()
        AND (skills IS NULL OR ARRAY_LENGTH(skills) = 0)
        
    UNION ALL
    
    SELECT 'duplicate_candidates' as metric_name,
           COUNT(*) - COUNT(DISTINCT candidate_url) as metric_value,
           CURRENT_DATE() as check_date
    FROM `{project_id}.rpo_structured_data.structured_candidates`
    WHERE DATE(structured_at) = CURRENT_DATE()
);
```

---

## ãƒ‡ãƒ¼ã‚¿ä¿æŒã¨ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–

### 1. è‡ªå‹•ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ã®è¨­å®š

```sql
-- ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ¬ãƒ™ãƒ«ã§ã®TTLè¨­å®š
ALTER TABLE `{project_id}.system_logs.application_logs`
SET OPTIONS (
    partition_expiration_days = 30,
    description = "Application logs with 30-day retention"
);

-- ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ã§ã®æœŸé™è¨­å®š
ALTER TABLE `{project_id}.rpo_raw_data.raw_candidates`
SET OPTIONS (partition_expiration_days = 90);
```

### 2. ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–æˆ¦ç•¥

```bash
#!/bin/bash
# archive_old_data.sh

PROJECT_ID="your-project-id"
ARCHIVE_BUCKET="gs://your-archive-bucket"
DATE_THRESHOLD="2023-01-01"

# å¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’Cloud Storageã«ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
bq extract \
    --destination_format=AVRO \
    --compression=SNAPPY \
    "${PROJECT_ID}:rpo_raw_data.raw_candidates\$${DATE_THRESHOLD//-/}" \
    "${ARCHIVE_BUCKET}/candidates/year=2023/candidates_*.avro"

# ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæˆåŠŸå¾Œã€BigQueryã‹ã‚‰å‰Šé™¤
if [ $? -eq 0 ]; then
    bq rm -f -t "${PROJECT_ID}:rpo_raw_data.raw_candidates\$${DATE_THRESHOLD//-/}"
    echo "Archived and removed partition for $DATE_THRESHOLD"
fi
```

---

## ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### ã‚ˆãã‚ã‚‹å•é¡Œã¨è§£æ±ºæ–¹æ³•

#### 1. æ¨©é™ã‚¨ãƒ©ãƒ¼
```
Error: Access Denied: BigQuery BigQuery: Permission denied
```
**è§£æ±ºæ–¹æ³•:** IAMæ¨©é™ã®ç¢ºèªã€ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã‚­ãƒ¼ã®æ›´æ–°

#### 2. ã‚¯ã‚¨ãƒªã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ
```
Error: Query exceeded timeout
```
**è§£æ±ºæ–¹æ³•:** ã‚¯ã‚¨ãƒªã®æœ€é©åŒ–ã€ãƒ‘ãƒ¼ãƒ†ã‚£ã‚·ãƒ§ãƒ³ä½¿ç”¨ã€LIMITå¥ã®è¿½åŠ 

#### 3. ã‚¹ã‚­ãƒ¼ãƒã‚¨ãƒ©ãƒ¼
```
Error: Invalid schema update
```
**è§£æ±ºæ–¹æ³•:** ãƒ‡ãƒ¼ã‚¿å‹ã®ç¢ºèªã€NULLABLEã®è¨­å®šç¢ºèª

è©³ç´°ãªãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã¯[ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã‚¬ã‚¤ãƒ‰](../operations/troubleshooting.md)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚

---

## æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

BigQueryã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãŒå®Œäº†ã—ãŸã‚‰ï¼š

1. [Supabaseã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](supabase.md) - ãƒ¡ã‚¤ãƒ³DBè¨­å®šã®ç¢ºèª
2. [ç’°å¢ƒè¨­å®š](environment.md) - çµ±åˆç’°å¢ƒå¤‰æ•°è¨­å®š
3. [APIä»•æ§˜æ›¸](../api/reference.md) - ãƒ‡ãƒ¼ã‚¿é€£æºAPIé–‹ç™º

## å‚è€ƒãƒªãƒ³ã‚¯

- [BigQueryå…¬å¼ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ](https://cloud.google.com/bigquery/docs)
- [BigQueryæ–™é‡‘](https://cloud.google.com/bigquery/pricing)
- [BigQueryãƒ™ã‚¹ãƒˆãƒ—ãƒ©ã‚¯ãƒ†ã‚£ã‚¹](https://cloud.google.com/bigquery/docs/best-practices-performance-overview)